---
title:          MPMoE :Memory Efficient MoE for Pre-Trained Models With Adaptive Pipeline Parallelism
date:           2024-04-08 00:01:00 +0800
selected:       true
pub:            "IEEE Transactions on Parallel and Distributed (TPDS)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-custom badge-secondary">Journal</span><span class="badge badge-pill badge-custom badge-danger">CCF-A</span><span class="badge badge-pill badge-custom badge-success">Best Paper Nomination</span>'
pub_date:       "2024"

abstract: >-
   In this paper, we present the design and implementation of MPMoE, a high-performance library that accelerates MoE training with adaptive and memory-efficient pipeline parallelism.
  
cover:          assets/images/covers/pub_tpds23_zz.png
authors:
  - Zheng Zhang
  - Yaqi Xia
  - Hulin Wang
  - Donglin Yang
  - Chuang Hu
  - Xiaobo Zhou
  - Dazhao Cheng
links:
  Paper: https://ieeexplore.ieee.org/abstract/document/10494556
  # Poster: assets/images/poster/icml2024-ws.jpeg
---
